{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51b9050-d66a-46d5-93f1-c17e736d607a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Understand the Domain and Problem Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b34402-4677-4833-bbf5-dd7fc0b4b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (140909, 24)\n",
      "Sample Records:\n",
      "       data          trip_creation_time  \\\n",
      "0  training  2018-09-20 02:35:36.476840   \n",
      "1  training  2018-09-20 02:35:36.476840   \n",
      "2  training  2018-09-20 02:35:36.476840   \n",
      "3  training  2018-09-20 02:35:36.476840   \n",
      "4  training  2018-09-20 02:35:36.476840   \n",
      "\n",
      "                                 route_schedule_uuid route_type  \\\n",
      "0  thanos::sroute:eb7bfc78-b351-4c0e-a951-fa3d5c3...    Carting   \n",
      "1  thanos::sroute:eb7bfc78-b351-4c0e-a951-fa3d5c3...    Carting   \n",
      "2  thanos::sroute:eb7bfc78-b351-4c0e-a951-fa3d5c3...    Carting   \n",
      "3  thanos::sroute:eb7bfc78-b351-4c0e-a951-fa3d5c3...    Carting   \n",
      "4  thanos::sroute:eb7bfc78-b351-4c0e-a951-fa3d5c3...    Carting   \n",
      "\n",
      "                 trip_uuid source_center                    source_name  \\\n",
      "0  trip-153741093647649320  IND388121AAA     Anand_VUNagar_DC (Gujarat)   \n",
      "1  trip-153741093647649320  IND388121AAA     Anand_VUNagar_DC (Gujarat)   \n",
      "2  trip-153741093647649320  IND388121AAA     Anand_VUNagar_DC (Gujarat)   \n",
      "3  trip-153741093647649320  IND388121AAA     Anand_VUNagar_DC (Gujarat)   \n",
      "4  trip-153741093647649320  IND388620AAB  Khambhat_MotvdDPP_D (Gujarat)   \n",
      "\n",
      "  destination_center               destination_name  \\\n",
      "0       IND388620AAB  Khambhat_MotvdDPP_D (Gujarat)   \n",
      "1       IND388620AAB  Khambhat_MotvdDPP_D (Gujarat)   \n",
      "2       IND388620AAB  Khambhat_MotvdDPP_D (Gujarat)   \n",
      "3       IND388620AAB  Khambhat_MotvdDPP_D (Gujarat)   \n",
      "4       IND388320AAA     Anand_Vaghasi_IP (Gujarat)   \n",
      "\n",
      "                od_start_time  ...     cutoff_timestamp  \\\n",
      "0  2018-09-20 03:21:32.418600  ...  2018-09-20 04:27:55   \n",
      "1  2018-09-20 03:21:32.418600  ...  2018-09-20 04:17:55   \n",
      "2  2018-09-20 03:21:32.418600  ...  2018-09-20 03:39:57   \n",
      "3  2018-09-20 03:21:32.418600  ...  2018-09-20 03:33:55   \n",
      "4  2018-09-20 04:47:45.236797  ...  2018-09-20 06:15:58   \n",
      "\n",
      "   actual_distance_to_destination  actual_time  osrm_time osrm_distance  \\\n",
      "0                       10.435660         14.0       11.0       11.9653   \n",
      "1                       18.936842         24.0       20.0       21.7243   \n",
      "2                       36.118028         62.0       40.0       45.5620   \n",
      "3                       39.386040         68.0       44.0       54.2181   \n",
      "4                       10.403038         15.0       11.0       12.1171   \n",
      "\n",
      "     factor  segment_actual_time  segment_osrm_time  segment_osrm_distance  \\\n",
      "0  1.272727                 14.0               11.0                11.9653   \n",
      "1  1.200000                 10.0                9.0                 9.7590   \n",
      "2  1.550000                 21.0               12.0                13.0224   \n",
      "3  1.545455                  6.0                5.0                 3.9153   \n",
      "4  1.363636                 15.0               11.0                12.1171   \n",
      "\n",
      "   segment_factor  \n",
      "0        1.272727  \n",
      "1        1.111111  \n",
      "2        1.750000  \n",
      "3        1.200000  \n",
      "4        1.363636  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Data Types and Missing Values:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140909 entries, 0 to 140908\n",
      "Data columns (total 24 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   data                            140909 non-null  object \n",
      " 1   trip_creation_time              140909 non-null  object \n",
      " 2   route_schedule_uuid             140909 non-null  object \n",
      " 3   route_type                      140909 non-null  object \n",
      " 4   trip_uuid                       140909 non-null  object \n",
      " 5   source_center                   140909 non-null  object \n",
      " 6   source_name                     140909 non-null  object \n",
      " 7   destination_center              140909 non-null  object \n",
      " 8   destination_name                140909 non-null  object \n",
      " 9   od_start_time                   140909 non-null  object \n",
      " 10  od_end_time                     140909 non-null  object \n",
      " 11  start_scan_to_end_scan          140909 non-null  float64\n",
      " 12  is_cutoff                       140909 non-null  bool   \n",
      " 13  cutoff_factor                   140909 non-null  int64  \n",
      " 14  cutoff_timestamp                140909 non-null  object \n",
      " 15  actual_distance_to_destination  140909 non-null  float64\n",
      " 16  actual_time                     140909 non-null  float64\n",
      " 17  osrm_time                       140909 non-null  float64\n",
      " 18  osrm_distance                   140909 non-null  float64\n",
      " 19  factor                          140909 non-null  float64\n",
      " 20  segment_actual_time             140909 non-null  float64\n",
      " 21  segment_osrm_time               140909 non-null  float64\n",
      " 22  segment_osrm_distance           140909 non-null  float64\n",
      " 23  segment_factor                  140909 non-null  float64\n",
      "dtypes: bool(1), float64(10), int64(1), object(12)\n",
      "memory usage: 24.9+ MB\n",
      "None\n",
      "\n",
      "Statistical Summary (Numerical Columns):\n",
      "       start_scan_to_end_scan  cutoff_factor  actual_distance_to_destination  \\\n",
      "count           140909.000000  140909.000000                   140909.000000   \n",
      "mean               982.636602     237.887906                      239.042070   \n",
      "std               1042.382580     347.816993                      348.051439   \n",
      "min                 21.000000       9.000000                        9.000045   \n",
      "25%                168.000000      22.000000                       23.381264   \n",
      "50%                472.000000      66.000000                       66.450476   \n",
      "75%               1686.000000     308.000000                      308.035783   \n",
      "max               7898.000000    1927.000000                     1927.447705   \n",
      "\n",
      "         actual_time      osrm_time  osrm_distance         factor  \\\n",
      "count  140909.000000  140909.000000  140909.000000  140909.000000   \n",
      "mean      424.627369     218.228140     290.762437       2.103093   \n",
      "std       603.310229     310.774552     424.893118       1.661662   \n",
      "min         9.000000       6.000000       9.008200       0.144000   \n",
      "25%        51.000000      27.000000      30.052700       1.602041   \n",
      "50%       135.000000      66.000000      80.557600       1.852941   \n",
      "75%       533.000000     269.000000     358.707400       2.200000   \n",
      "max      4154.000000    1686.000000    2326.199100      76.500000   \n",
      "\n",
      "       segment_actual_time  segment_osrm_time  segment_osrm_distance  \\\n",
      "count        140909.000000      140909.000000          140909.000000   \n",
      "mean             35.820735          18.631230              23.017969   \n",
      "std              50.473058          14.726216              17.839862   \n",
      "min            -244.000000           0.000000               0.000000   \n",
      "25%              20.000000          11.000000              12.272200   \n",
      "50%              28.000000          17.000000              23.602800   \n",
      "75%              40.000000          22.000000              27.898500   \n",
      "max            3051.000000        1611.000000            2191.403700   \n",
      "\n",
      "       segment_factor  \n",
      "count   140909.000000  \n",
      "mean         2.159147  \n",
      "std          4.328712  \n",
      "min        -23.444444  \n",
      "25%          1.346154  \n",
      "50%          1.666667  \n",
      "75%          2.222222  \n",
      "max        558.500000  \n",
      "\n",
      "Unique Values in Categorical Columns:\n",
      "data: 2 unique values\n",
      "trip_creation_time: 14681 unique values\n",
      "route_schedule_uuid: 1490 unique values\n",
      "route_type: 2 unique values\n",
      "trip_uuid: 14681 unique values\n",
      "source_center: 1478 unique values\n",
      "source_name: 1478 unique values\n",
      "destination_center: 1451 unique values\n",
      "destination_name: 1451 unique values\n",
      "od_start_time: 25714 unique values\n",
      "od_end_time: 25714 unique values\n",
      "cutoff_timestamp: 89862 unique values\n",
      "\n",
      "Datetime Columns:\n",
      "trip_creation_time:\n",
      "0    2018-09-20 02:35:36.476840\n",
      "1    2018-09-20 02:35:36.476840\n",
      "2    2018-09-20 02:35:36.476840\n",
      "3    2018-09-20 02:35:36.476840\n",
      "4    2018-09-20 02:35:36.476840\n",
      "Name: trip_creation_time, dtype: object\n",
      "od_start_time:\n",
      "0    2018-09-20 03:21:32.418600\n",
      "1    2018-09-20 03:21:32.418600\n",
      "2    2018-09-20 03:21:32.418600\n",
      "3    2018-09-20 03:21:32.418600\n",
      "4    2018-09-20 04:47:45.236797\n",
      "Name: od_start_time, dtype: object\n",
      "od_end_time:\n",
      "0    2018-09-20 04:47:45.236797\n",
      "1    2018-09-20 04:47:45.236797\n",
      "2    2018-09-20 04:47:45.236797\n",
      "3    2018-09-20 04:47:45.236797\n",
      "4    2018-09-20 06:36:55.627764\n",
      "Name: od_end_time, dtype: object\n",
      "cutoff_timestamp:\n",
      "0    2018-09-20 04:27:55\n",
      "1    2018-09-20 04:17:55\n",
      "2    2018-09-20 03:39:57\n",
      "3    2018-09-20 03:33:55\n",
      "4    2018-09-20 06:15:58\n",
      "Name: cutoff_timestamp, dtype: object\n",
      "actual_time:\n",
      "0    14.0\n",
      "1    24.0\n",
      "2    62.0\n",
      "3    68.0\n",
      "4    15.0\n",
      "Name: actual_time, dtype: float64\n",
      "osrm_time:\n",
      "0    11.0\n",
      "1    20.0\n",
      "2    40.0\n",
      "3    44.0\n",
      "4    11.0\n",
      "Name: osrm_time, dtype: float64\n",
      "segment_actual_time:\n",
      "0    14.0\n",
      "1    10.0\n",
      "2    21.0\n",
      "3     6.0\n",
      "4    15.0\n",
      "Name: segment_actual_time, dtype: float64\n",
      "segment_osrm_time:\n",
      "0    11.0\n",
      "1     9.0\n",
      "2    12.0\n",
      "3     5.0\n",
      "4    11.0\n",
      "Name: segment_osrm_time, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"clean_dataset.csv\")\n",
    "\n",
    "# Shape of the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Sample Records:\")\n",
    "print(df.head())\n",
    "\n",
    "# Summary of data types and missing values\n",
    "print(\"\\nData Types and Missing Values:\")\n",
    "print(df.info())\n",
    "\n",
    "# Basic statistics for numeric columns\n",
    "print(\"\\nStatistical Summary (Numerical Columns):\")\n",
    "print(df.describe())\n",
    "\n",
    "# Unique values in each column (for categorical understanding)\n",
    "print(\"\\nUnique Values in Categorical Columns:\")\n",
    "for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Preview unique values for date columns if present\n",
    "print(\"\\nDatetime Columns:\")\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower() or 'time' in col.lower():\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa1fb6-e491-4494-b9bf-048cb94a5ff4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9153f4-723f-49c1-a3fc-e6b5475c2d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features to drop: ['actual_distance_to_destination', 'actual_time', 'osrm_time', 'osrm_distance', 'segment_osrm_distance']\n",
      "\n",
      "VIF Scores:\n",
      "                  feature       VIF\n",
      "4  start_scan_to_end_scan  2.914135\n",
      "5           cutoff_factor  2.606873\n",
      "7     segment_actual_time  2.200595\n",
      "6                  factor  1.713115\n",
      "8       segment_osrm_time  1.653010\n",
      "9          segment_factor  1.638216\n",
      "1              route_type  1.491649\n",
      "2           source_center  1.204978\n",
      "3      destination_center  1.172240\n",
      "0                    data  1.003509\n",
      "\n",
      "Top features by Mutual Information:\n",
      "cutoff_factor             0.352854\n",
      "segment_osrm_time         0.135933\n",
      "segment_actual_time       0.091274\n",
      "segment_factor            0.081164\n",
      "start_scan_to_end_scan    0.060538\n",
      "destination_center        0.059472\n",
      "source_center             0.057026\n",
      "route_type                0.036407\n",
      "factor                    0.032138\n",
      "data                      0.021833\n",
      "dtype: float64\n",
      "\n",
      "Top features by Random Forest:\n",
      "cutoff_factor             0.280987\n",
      "segment_osrm_time         0.226923\n",
      "segment_actual_time       0.132989\n",
      "start_scan_to_end_scan    0.101541\n",
      "segment_factor            0.062454\n",
      "route_type                0.053079\n",
      "factor                    0.046781\n",
      "source_center             0.045896\n",
      "destination_center        0.045465\n",
      "data                      0.003884\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LassoCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"clean_dataset.csv\")\n",
    "\n",
    "# Target for classification\n",
    "target = 'is_cutoff'\n",
    "\n",
    "# Drop ID/time columns (to be engineered separately if needed)\n",
    "drop_cols = [\n",
    "    'trip_uuid', 'route_schedule_uuid', 'trip_creation_time',\n",
    "    'od_start_time', 'od_end_time', 'cutoff_timestamp',\n",
    "    'source_name', 'destination_name'\n",
    "]\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# Encode categoricals\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Prepare features\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# 1. Correlation matrix to detect multicollinearity\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "print(\"Highly correlated features to drop:\", high_corr_features)\n",
    "X = X.drop(columns=high_corr_features)\n",
    "\n",
    "# 2. VIF check\n",
    "scaler = StandardScaler ()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"feature\"] = X.columns\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
    "print(\"\\nVIF Scores:\")\n",
    "print(vif_df.sort_values(by=\"VIF\", ascending=False))\n",
    "\n",
    "# Optional: Drop features with VIF > 10 (multicollinearity)\n",
    "X = X.loc[:, vif_df[vif_df[\"VIF\"] < 10][\"feature\"]]\n",
    "\n",
    "# 3. Feature Importance - Mutual Information (for classification)\n",
    "mi = mutual_info_classif(X, y, discrete_features='auto')\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop features by Mutual Information:\")\n",
    "print(mi_series)\n",
    "\n",
    "# 4. Feature Importance - Random Forest\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X, y)\n",
    "rf_importances = pd.Series(model_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop features by Random Forest:\")\n",
    "print(rf_importances)\n",
    "\n",
    "# Optional 5. LassoCV for regression-based selection (if using regression target like 'actual_time')\n",
    "# from sklearn.linear_model import LassoCV\n",
    "# lasso_model = LassoCV(cv=5).fit(X, df['actual_time'])\n",
    "# lasso_coef = pd.Series(lasso_model.coef_, index=X.columns)\n",
    "# print(\"\\nNon-zero Lasso Coefficients:\")\n",
    "# print(lasso_coef[lasso_coef != 0].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0112a5-a2e3-46f5-9a50-c925081e7a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4482e7-6f61-4f6a-aca4-3c385e1c6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"clean_dataset.csv\")\n",
    "\n",
    "# --- Step 1: Parse datetime fields ---\n",
    "datetime_cols = ['trip_creation_time', 'od_start_time', 'od_end_time', 'cutoff_timestamp']\n",
    "for col in datetime_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Extract time-based features\n",
    "df['trip_creation_hour'] = df['trip_creation_time'].dt.hour\n",
    "df['trip_creation_day'] = df['trip_creation_time'].dt.day\n",
    "df['trip_creation_weekday'] = df['trip_creation_time'].dt.weekday\n",
    "df['od_start_hour'] = df['od_start_time'].dt.hour\n",
    "df['od_end_hour'] = df['od_end_time'].dt.hour\n",
    "df['cutoff_hour'] = df['cutoff_timestamp'].dt.hour\n",
    "\n",
    "# Trip duration features\n",
    "df['planned_duration'] = (df['od_end_time'] - df['od_start_time']).dt.total_seconds() / 60\n",
    "df['creation_to_start_mins'] = (df['od_start_time'] - df['trip_creation_time']).dt.total_seconds() / 60\n",
    "df['start_to_cutoff_mins'] = (df['cutoff_timestamp'] - df['od_start_time']).dt.total_seconds() / 60\n",
    "\n",
    "# --- Step 2: Mathematical feature engineering ---\n",
    "# Ratios\n",
    "df['actual_vs_osrm_time'] = df['actual_time'] / df['osrm_time']\n",
    "df['segment_actual_vs_osrm_time'] = df['segment_actual_time'] / df['segment_osrm_time']\n",
    "df['distance_per_min'] = df['actual_distance_to_destination'] / df['actual_time']\n",
    "\n",
    "# Differences\n",
    "df['time_difference'] = df['actual_time'] - df['osrm_time']\n",
    "df['segment_time_diff'] = df['segment_actual_time'] - df['segment_osrm_time']\n",
    "\n",
    "# --- Step 3: Aggregations (example) ---\n",
    "# Trip counts per route (if route info is retained)\n",
    "# df['route_trip_count'] = df.groupby('route_schedule_uuid')['trip_uuid'].transform('count')\n",
    "\n",
    "# Trip counts per center pair (origin-destination)\n",
    "df['center_pair'] = df['source_center'] + \"_\" + df['destination_center']\n",
    "df['center_pair_count'] = df.groupby('center_pair')['trip_uuid'].transform('count')\n",
    "\n",
    "# --- Step 4: Domain-specific features (logistics) ---\n",
    "# High-delay indicator\n",
    "df['is_heavy_delay'] = (df['actual_vs_osrm_time'] > 1.5).astype(int)\n",
    "\n",
    "# Delay bucket (optional)\n",
    "df['delay_category'] = pd.cut(df['actual_vs_osrm_time'], bins=[0, 1, 1.25, 1.5, np.inf],\n",
    "                              labels=['on_time', 'slight_delay', 'delay', 'heavy_delay'])\n",
    "\n",
    "# --- Drop helper columns if not needed ---\n",
    "df = df.drop(columns=['center_pair'])\n",
    "\n",
    "# Save to file if needed\n",
    "\n",
    "\n",
    "# Save the final feature-engineered dataset\n",
    "df.to_csv(\"feature_engineered_dataset.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfb913-a035-493b-b20c-e629c309d16b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5465870-54d6-4176-be57-285c607c36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler, StandardScaler, QuantileTransformer,\n",
    "    FunctionTransformer, PolynomialFeatures\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"feature_engineered_dataset.csv\")\n",
    "\n",
    "# Drop non-numeric and ID/date columns\n",
    "exclude_cols = ['trip_uuid', 'route_schedule_uuid', 'trip_creation_time',\n",
    "                'od_start_time', 'od_end_time', 'cutoff_timestamp', \n",
    "                'source_center', 'destination_center', 'source_name', \n",
    "                'destination_name', 'delay_category']\n",
    "features = df.drop(columns=[col for col in exclude_cols if col in df.columns])\n",
    "\n",
    "# Separate target\n",
    "if 'is_cutoff' in features.columns:\n",
    "    target = features['is_cutoff']\n",
    "    features = features.drop(columns=['is_cutoff'])\n",
    "else:\n",
    "    target = None\n",
    "\n",
    "# Replace inf/-inf with NaN and drop them\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features.dropna(inplace=True)\n",
    "\n",
    "# Drop target rows to match\n",
    "if target is not None:\n",
    "    target = target.loc[features.index]\n",
    "\n",
    "# Select numeric columns\n",
    "num_cols = features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Filter log-safe columns (positive only and no zero)\n",
    "log_cols = [col for col in num_cols if (features[col] > 0).all()]\n",
    "\n",
    "# Apply transformations (individually to avoid conflicts)\n",
    "transformed_dfs = {}\n",
    "\n",
    "# 1. MinMax Scaling\n",
    "minmax_scaled = MinMaxScaler().fit_transform(features[num_cols])\n",
    "transformed_dfs[\"minmax\"] = pd.DataFrame(minmax_scaled, columns=[f\"{col}_minmax\" for col in num_cols])\n",
    "\n",
    "# 2. Standard Scaling\n",
    "standard_scaled = StandardScaler().fit_transform(features[num_cols])\n",
    "transformed_dfs[\"standard\"] = pd.DataFrame(standard_scaled, columns=[f\"{col}_std\" for col in num_cols])\n",
    "\n",
    "# 3. Log Transform (safe columns only)\n",
    "log_transformed = np.log1p(features[log_cols])\n",
    "transformed_dfs[\"log\"] = log_transformed.rename(columns=lambda c: f\"{c}_log\")\n",
    "\n",
    "# 4. Quantile Transformer\n",
    "quantile_scaled = QuantileTransformer(output_distribution='normal').fit_transform(features[num_cols])\n",
    "transformed_dfs[\"quantile\"] = pd.DataFrame(quantile_scaled, columns=[f\"{col}_qt\" for col in num_cols])\n",
    "\n",
    "# 5. Polynomial Features (up to degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(features[num_cols])\n",
    "poly_feature_names = poly.get_feature_names_out(num_cols)\n",
    "transformed_dfs[\"poly\"] = pd.DataFrame(poly_features, columns=[f\"{name}_poly\" for name in poly_feature_names])\n",
    "\n",
    "# Merge all\n",
    "df_transformed = pd.concat(transformed_dfs.values(), axis=1)\n",
    "\n",
    "# Add target back if available\n",
    "if target is not None:\n",
    "    df_transformed['is_cutoff'] = target.reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "df_transformed.to_csv(\"transformed_features_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd3edd-503e-4027-951c-42a204274e6a",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c670a590-e168-4d74-bde1-34d9a8254c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
    ")\n",
    "\n",
    "# Load the feature-engineered dataset\n",
    "df = pd.read_csv(\"feature_engineered_dataset.csv\")\n",
    "\n",
    "# Drop non-numeric, ID, and datetime columns\n",
    "exclude_cols = ['trip_uuid', 'route_schedule_uuid', 'trip_creation_time',\n",
    "                'od_start_time', 'od_end_time', 'cutoff_timestamp', \n",
    "                'source_center', 'destination_center', 'source_name', \n",
    "                'destination_name', 'delay_category']\n",
    "\n",
    "features = df.drop(columns=[col for col in exclude_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "# Separate target\n",
    "if 'is_cutoff' in features.columns:\n",
    "    target = features['is_cutoff']\n",
    "    features = features.drop(columns=['is_cutoff'])\n",
    "else:\n",
    "    target = None\n",
    "\n",
    "# Replace infinite values and drop NaNs\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features.dropna(inplace=True)\n",
    "if target is not None:\n",
    "    target = target.loc[features.index]\n",
    "\n",
    "# Select only numeric columns\n",
    "num_cols = features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Initialize scalers\n",
    "scalers = {\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"maxabs\": MaxAbsScaler()\n",
    "}\n",
    "\n",
    "# Apply each scaler and store transformed DataFrames\n",
    "scaled_dfs = {}\n",
    "for name, scaler in scalers.items():\n",
    "    scaled = scaler.fit_transform(features[num_cols])\n",
    "    scaled_dfs[name] = pd.DataFrame(scaled, columns=[f\"{col}_{name}\" for col in num_cols])\n",
    "\n",
    "# Merge all scaled features\n",
    "scaled_all = pd.concat(scaled_dfs.values(), axis=1)\n",
    "\n",
    "# Add target back if present\n",
    "if target is not None:\n",
    "    scaled_all['is_cutoff'] = target.reset_index(drop=True)\n",
    "\n",
    "# Save the fully scaled dataset\n",
    "scaled_all.to_csv(\"scaled_features_dataset.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613c036-61d9-4ef4-b6fb-1c1279e647ac",
   "metadata": {},
   "source": [
    "## 6. Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f62ade-f21a-4fcf-8365-77760e8c96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PCA reduced feature count: 7\n",
      "✅ ANOVA top 30: ['start_scan_to_end_scan_minmax', 'cutoff_factor_minmax', 'actual_distance_to_destination_minmax', 'osrm_distance_minmax', 'segment_osrm_time_minmax', 'segment_osrm_distance_minmax', 'planned_duration_minmax', 'start_to_cutoff_mins_minmax', 'distance_per_min_minmax', 'center_pair_count_minmax', 'start_scan_to_end_scan_robust', 'cutoff_factor_robust', 'actual_distance_to_destination_robust', 'osrm_distance_robust', 'segment_osrm_time_robust', 'segment_osrm_distance_robust', 'planned_duration_robust', 'start_to_cutoff_mins_robust', 'distance_per_min_robust', 'center_pair_count_robust', 'start_scan_to_end_scan_maxabs', 'cutoff_factor_maxabs', 'actual_distance_to_destination_maxabs', 'osrm_distance_maxabs', 'segment_osrm_time_maxabs', 'segment_osrm_distance_maxabs', 'planned_duration_maxabs', 'start_to_cutoff_mins_maxabs', 'distance_per_min_maxabs', 'center_pair_count_maxabs']\n",
      "✅ Chi-Square top 30: ['start_scan_to_end_scan_minmax', 'cutoff_factor_minmax', 'actual_distance_to_destination_minmax', 'actual_time_minmax', 'osrm_time_minmax', 'osrm_distance_minmax', 'od_end_hour_minmax', 'planned_duration_minmax', 'start_to_cutoff_mins_minmax', 'center_pair_count_minmax', 'start_scan_to_end_scan_robust', 'cutoff_factor_robust', 'actual_distance_to_destination_robust', 'actual_time_robust', 'osrm_time_robust', 'osrm_distance_robust', 'od_end_hour_robust', 'planned_duration_robust', 'start_to_cutoff_mins_robust', 'center_pair_count_robust', 'start_scan_to_end_scan_maxabs', 'cutoff_factor_maxabs', 'actual_distance_to_destination_maxabs', 'actual_time_maxabs', 'osrm_time_maxabs', 'osrm_distance_maxabs', 'od_end_hour_maxabs', 'planned_duration_maxabs', 'start_to_cutoff_mins_maxabs', 'center_pair_count_maxabs']\n",
      "✅ Lasso-selected: ['start_scan_to_end_scan_robust', 'actual_distance_to_destination_robust', 'actual_time_robust', 'osrm_time_robust', 'osrm_distance_robust', 'factor_robust', 'segment_actual_time_robust', 'segment_osrm_distance_robust', 'segment_factor_robust', 'trip_creation_hour_robust', 'trip_creation_day_robust', 'trip_creation_weekday_robust', 'od_start_hour_robust', 'od_end_hour_robust', 'cutoff_hour_robust', 'planned_duration_robust', 'creation_to_start_mins_robust', 'start_to_cutoff_mins_robust', 'actual_vs_osrm_time_robust', 'segment_actual_vs_osrm_time_robust', 'distance_per_min_robust', 'time_difference_robust', 'segment_time_diff_robust', 'center_pair_count_robust', 'is_heavy_delay_robust']\n",
      "✅ RFE top 30: ['cutoff_factor_minmax', 'actual_distance_to_destination_minmax', 'osrm_time_minmax', 'osrm_distance_minmax', 'segment_factor_minmax', 'trip_creation_hour_minmax', 'trip_creation_day_minmax', 'start_to_cutoff_mins_minmax', 'segment_actual_vs_osrm_time_minmax', 'start_scan_to_end_scan_robust', 'cutoff_factor_robust', 'actual_distance_to_destination_robust', 'actual_time_robust', 'osrm_time_robust', 'osrm_distance_robust', 'segment_osrm_distance_robust', 'trip_creation_hour_robust', 'trip_creation_day_robust', 'planned_duration_robust', 'start_to_cutoff_mins_robust', 'time_difference_robust', 'center_pair_count_robust', 'is_heavy_delay_robust', 'cutoff_factor_maxabs', 'actual_distance_to_destination_maxabs', 'osrm_time_maxabs', 'osrm_distance_maxabs', 'trip_creation_hour_maxabs', 'trip_creation_day_maxabs', 'start_to_cutoff_mins_maxabs']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Config\n",
    "DATA_PATH = \"scaled_features_dataset.csv\"\n",
    "TARGET_COL = 'is_cutoff'\n",
    "SAMPLE_ROWS = None  # Set to e.g., 10000 for quick testing\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH, usecols=lambda c: c == TARGET_COL or \"_std\" in c or \"_minmax\" in c or \"_robust\" in c or \"_maxabs\" in c)\n",
    "if SAMPLE_ROWS:\n",
    "    df = df.sample(n=SAMPLE_ROWS, random_state=42)\n",
    "\n",
    "# Split features and target\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "# Impute missing values\n",
    "X = pd.DataFrame(SimpleImputer(strategy='mean').fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 1. PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"✅ PCA reduced feature count:\", X_pca.shape[1])\n",
    "\n",
    "# 2. ANOVA F-test (top 30)\n",
    "anova_features = X.columns[SelectKBest(score_func=f_classif, k=30).fit(X, y).get_support()]\n",
    "print(\"✅ ANOVA top 30:\", list(anova_features))\n",
    "\n",
    "# 3. Chi-Square (requires non-negative input)\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi_features = X.columns[SelectKBest(score_func=chi2, k=30).fit(X_chi, y).get_support()]\n",
    "print(\"✅ Chi-Square top 30:\", list(chi_features))\n",
    "\n",
    "# 4. LassoCV (embedded method)\n",
    "lasso = LassoCV(cv=3, max_iter=10000, n_jobs=-1, random_state=42).fit(X, y)\n",
    "lasso_features = X.columns[lasso.coef_ != 0]\n",
    "print(\"✅ Lasso-selected:\", list(lasso_features))\n",
    "\n",
    "# 5. Recursive Feature Elimination\n",
    "rfe_model = LogisticRegression(max_iter=500, solver='liblinear')\n",
    "rfe = RFE(rfe_model, n_features_to_select=30, step=10)\n",
    "rfe_features = X.columns[rfe.fit(X, y).support_]\n",
    "print(\"✅ RFE top 30:\", list(rfe_features))\n",
    "\n",
    "# Optional: Save PCA output\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f'pca_{i+1}' for i in range(X_pca.shape[1])])\n",
    "pca_df[TARGET_COL] = y.reset_index(drop=True)\n",
    "pca_df.to_csv(\"pca_reduced_dataset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
